"""MLflowã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç®¡ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
import tempfile
import joblib
import json
import os
import mlflow


def save_model_artifacts(pipeline, feature_names, target_names, cfg):
    """ãƒ¢ãƒ‡ãƒ«ã¨é–¢é€£æƒ…å ±ã‚’MLflowã«ä¿å­˜ï¼ˆconfigé§†å‹•ã€ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†çµ±ä¸€ï¼‰"""
    with tempfile.TemporaryDirectory() as tmp_dir:
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆconfigåŒ–ï¼‰
        model_path = os.path.join(tmp_dir, cfg.mlflow.artifacts.model_filename)
        joblib.dump(pipeline, model_path)
        mlflow.log_artifact(model_path, cfg.mlflow.artifacts.model_dir)

        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜ï¼ˆconfigåŒ–ï¼‰
        info_path = os.path.join(tmp_dir, cfg.mlflow.artifacts.info_filename)
        with open(info_path, 'w') as f:
            json.dump({
                "model_type": type(pipeline.named_steps['classifier']).__name__,
                "feature_names": list(feature_names),
                "target_names": [str(x) for x in target_names]
            }, f)
        mlflow.log_artifact(info_path, cfg.mlflow.artifacts.model_dir)


def log_config_parameters(cfg):
    """å®Ÿé¨“è¨­å®šå…¨ä½“ã‚’MLflowã«è¨˜éŒ²ï¼ˆæœ€çŸ­ç‰ˆï¼‰"""
    from omegaconf import OmegaConf
    for key, value in OmegaConf.to_container(cfg, resolve=True).items():
        mlflow.log_param(key, str(value))


def log_runtime_parameters(pipeline, cfg, best_params=None):
    """å®Ÿè¡Œæ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’MLflowã«è¨˜éŒ²ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰"""
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆæƒ…å ±
    mlflow.log_param("pipeline_steps", len(pipeline.steps))
    for i, (step_name, step_obj) in enumerate(pipeline.steps):
        mlflow.log_param(f"step_{i}_name", step_name)
        mlflow.log_param(f"step_{i}_class", type(step_obj).__name__)

    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¸¸ã«è¨˜éŒ²ï¼‰
    model = pipeline.named_steps.get('classifier')
    if model:
        # ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
        model_params = model.get_params()
        for key, value in model_params.items():
            mlflow.log_param(f"model_{key}", str(value))

        # Optunaæœ€é©åŒ–æƒ…å ±
        mlflow.log_param("tuning_optimized", best_params is not None)
        if best_params:
            for key, value in best_params.items():
                mlflow.log_param(f"tuned_{key}", str(value))


def log_experiment_metrics(best_pipeline, X_train, y_train, X_test, y_test, task_type, cv_scores, cfg=None, y_pred=None):
    """å®Ÿé¨“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’MLflowã«è¨˜éŒ²"""
    import numpy as np
    from sklearn.metrics import (
        accuracy_score, f1_score, roc_auc_score, precision_score, recall_score,
        mean_squared_error, mean_absolute_error, r2_score
    )

    # CVçµæœè¨˜éŒ²
    mlflow.log_metric("cv_mean", cv_scores.mean())
    mlflow.log_metric("cv_std", cv_scores.std())

    # Testè©•ä¾¡ï¼ˆäºˆæ¸¬çµæœãŒæ¸¡ã•ã‚Œãªã„å ´åˆã®ã¿å®Ÿè¡Œï¼‰
    if y_pred is None:
        y_pred = best_pipeline.predict(X_test)

    if task_type == "classification":
        # åˆ†é¡è©•ä¾¡æŒ‡æ¨™
        test_accuracy = accuracy_score(y_test, y_pred)
        mlflow.log_metric("test_accuracy", test_accuracy)

        # configè¨­å®šã«åŸºã¥ããƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        scoring_type = cfg.optuna.scoring.classification if cfg else "f1_weighted"
        is_binary = len(np.unique(y_test)) == 2

        # F1ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆconfigè¨­å®šã«å¯¾å¿œï¼‰
        if scoring_type in ["f1", "f1_binary"] and is_binary:
            # äºŒå€¤åˆ†é¡: binaryãƒ¢ãƒ¼ãƒ‰ï¼ˆé™½æ€§ã‚¯ãƒ©ã‚¹ã®F1ï¼‰
            test_f1 = f1_score(y_test, y_pred, pos_label=1)
            metric_suffix = "binary"
        elif scoring_type == "f1_macro":
            test_f1 = f1_score(y_test, y_pred, average='macro')
            metric_suffix = "macro"
        else:  # f1_weighted or default
            test_f1 = f1_score(y_test, y_pred, average='weighted')
            metric_suffix = "weighted"

        mlflow.log_metric(f"test_f1_{metric_suffix}", test_f1)

        # AUCï¼ˆäºŒå€¤ãƒ»å¤šå€¤åˆ†é¡å¯¾å¿œï¼‰
        if hasattr(best_pipeline, 'predict_proba'):
            y_proba = best_pipeline.predict_proba(X_test)
            if is_binary:  # äºŒå€¤åˆ†é¡
                test_auc = roc_auc_score(y_test, y_proba[:, 1])
            else:  # å¤šå€¤åˆ†é¡
                test_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')
            mlflow.log_metric("test_auc", test_auc)

        # Precision/Recallï¼ˆconfigã¨åŒã˜averageã‚’ä½¿ç”¨ï¼‰
        if scoring_type in ["f1", "f1_binary", "precision", "recall"] and is_binary:
            test_precision = precision_score(y_test, y_pred, pos_label=1)
            test_recall = recall_score(y_test, y_pred, pos_label=1)
            mlflow.log_metric("test_precision_binary", test_precision)
            mlflow.log_metric("test_recall_binary", test_recall)
        elif "macro" in scoring_type:
            test_precision = precision_score(y_test, y_pred, average='macro')
            test_recall = recall_score(y_test, y_pred, average='macro')
            mlflow.log_metric("test_precision_macro", test_precision)
            mlflow.log_metric("test_recall_macro", test_recall)
        else:
            test_precision = precision_score(y_test, y_pred, average='weighted')
            test_recall = recall_score(y_test, y_pred, average='weighted')
            mlflow.log_metric("test_precision_weighted", test_precision)
            mlflow.log_metric("test_recall_weighted", test_recall)

        # è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«
        f1_label = "F1" if metric_suffix == "binary" else f"F1_{metric_suffix[:3]}"
        print(f"ğŸš€ {mlflow.active_run().info.run_id[:8]} | ğŸ“ˆ CV: {cv_scores.mean():.3f}Â±{cv_scores.std():.3f} Test Acc: {test_accuracy:.3f} {f1_label}: {test_f1:.3f} AUC: {test_auc:.3f}")

    else:  # regression
        # å›å¸°è©•ä¾¡æŒ‡æ¨™
        test_mse = mean_squared_error(y_test, y_pred)
        test_rmse = np.sqrt(test_mse)
        test_mae = mean_absolute_error(y_test, y_pred)
        test_r2 = r2_score(y_test, y_pred)

        mlflow.log_metric("test_mse", test_mse)
        mlflow.log_metric("test_rmse", test_rmse)
        mlflow.log_metric("test_mae", test_mae)
        mlflow.log_metric("test_r2", test_r2)

        # Trainè©•ä¾¡ï¼ˆå›å¸°ã§ã¯æ¯”è¼ƒç”¨ï¼‰
        y_train_pred = best_pipeline.predict(X_train)
        train_r2 = r2_score(y_train, y_train_pred)
        mlflow.log_metric("train_r2", train_r2)

        print(f"ğŸš€ {mlflow.active_run().info.run_id[:8]} | ğŸ“ˆ CV: {cv_scores.mean():.3f}Â±{cv_scores.std():.3f} Test RMSE: {test_rmse:.3f} RÂ²: {test_r2:.3f}")


def create_prediction_dataframe(pipeline, X_test, y_test, task_type, y_pred=None):
    """äºˆæ¸¬çµæœã¨å¤‰æ›æ¸ˆã¿ç‰¹å¾´é‡ã‚’çµ±åˆã—ãŸDataFrameã‚’ä½œæˆ"""
    import pandas as pd
    import numpy as np

    # äºˆæ¸¬å€¤ã¨äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—ï¼ˆäºˆæ¸¬çµæœãŒæ¸¡ã•ã‚Œãªã„å ´åˆã®ã¿å®Ÿè¡Œï¼‰
    if y_pred is None:
        y_pred = pipeline.predict(X_test)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤‰æ›å¾Œã®ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆæœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—å‰ã¾ã§ï¼‰
    try:
        # ImbPipelineã¾ãŸã¯é€šå¸¸ã®Pipelineã§ã®transformå‡¦ç†
        X_test_transformed = pipeline[:-1].transform(X_test)
    except AttributeError:
        # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ãŒå«ã¾ã‚Œã‚‹å ´åˆã®æ‰‹å‹•å¤‰æ›
        from src.mlops.components.pipeline import SAMPLING_CLASSES
        X_current = X_test.copy()
        for name, transformer in pipeline.steps[:-1]:
            # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¯transformãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒãŸãªã„ãŸã‚ã€ãƒ†ã‚¹ãƒˆæ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if any(cls in str(type(transformer)) for cls in SAMPLING_CLASSES):
                continue
            X_current = transformer.transform(X_current)
        X_test_transformed = X_current

    # å¤‰æ›å¾Œãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
    if isinstance(X_test_transformed, pd.DataFrame):
        df_transformed = X_test_transformed.copy()
    else:
        # numpy arrayã®å ´åˆï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤‰æ›å¾Œã®æ­£ã—ã„ç‰¹å¾´é‡åã‚’ä½¿ç”¨ï¼‰
        from src.utils.pipeline_utils import get_pipeline_feature_names
        original_feature_names = (X_test.columns.tolist()
                                if hasattr(X_test, 'columns')
                                else [f"feature_{i}" for i in range(X_test.shape[1])])
        transformed_feature_names = get_pipeline_feature_names(pipeline, original_feature_names)

        n_features = X_test_transformed.shape[1]
        # å¤‰æ›å¾Œã®ç‰¹å¾´é‡æ•°ã«åˆã‚ã›ã¦èª¿æ•´
        if len(transformed_feature_names) >= n_features:
            feature_names = transformed_feature_names[:n_features]
        else:
            feature_names = transformed_feature_names + [f"generated_feature_{i}" for i in range(len(transformed_feature_names), n_features)]

        df_transformed = pd.DataFrame(X_test_transformed, columns=feature_names, index=X_test.index)

    # å…ƒã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡ã‚«ãƒ©ãƒ ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å´ã‚’å„ªå…ˆï¼‰
    df_result = X_test.copy()

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤‰æ›å¾Œã®ç‰¹å¾´é‡ã§ä¸Šæ›¸ã
    for col in df_transformed.columns:
        if col in df_result.columns:
            # é‡è¤‡ã‚«ãƒ©ãƒ ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å´ã‚’æ¡ç”¨
            df_result[col] = df_transformed[col].values
        else:
            # æ–°è¦ç”Ÿæˆã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
            df_result[f"generated_{col}"] = df_transformed[col].values

    # å®Ÿéš›ã®å€¤ã¨äºˆæ¸¬å€¤ã‚’è¿½åŠ 
    df_result['y_true'] = y_test.values
    df_result['y_pred'] = y_pred

    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦äºˆæ¸¬ç¢ºç‡ã‚’è¿½åŠ 
    if task_type == "classification" and hasattr(pipeline, 'predict_proba'):
        y_proba = pipeline.predict_proba(X_test)
        n_classes = y_proba.shape[1]

        # å„ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ã‚’è¿½åŠ 
        for i in range(n_classes):
            df_result[f'proba_class_{i}'] = y_proba[:, i]

        # äºˆæ¸¬ã®ä¿¡é ¼åº¦ï¼ˆæœ€å¤§ç¢ºç‡ï¼‰
        df_result['prediction_confidence'] = y_proba.max(axis=1)

    return df_result


def save_prediction_results(df_predictions, cfg):
    """äºˆæ¸¬çµæœDataFrameã‚’MLflowã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜"""
    import tempfile
    import os

    with tempfile.TemporaryDirectory() as tmp_dir:
        # CSVä¿å­˜
        csv_path = os.path.join(tmp_dir, "test_predictions.csv")
        df_predictions.to_csv(csv_path, index=False)

        # MLflowã«ä¿å­˜
        mlflow.log_artifact(csv_path, "predictions")

        print(f"ğŸ’¾ äºˆæ¸¬çµæœCSVä¿å­˜å®Œäº†: predictions/test_predictions.csv")
        print(f"   - ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_predictions)}ä»¶")
        print(f"   - ã‚«ãƒ©ãƒ æ•°: {len(df_predictions.columns)}åˆ—")

    return df_predictions


def setup_mlflow_experiment(cfg):
    """MLflowå®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    mlflow.set_experiment(cfg.mlflow.experiment_name)


def set_mlflow_tags(cfg):
    """MLflowã‚¿ã‚°è¨­å®šï¼ˆruné–‹å§‹å¾Œã«å‘¼ã³å‡ºã—ï¼‰"""
    if hasattr(cfg.mlflow, 'tags'):
        for key, value in cfg.mlflow.tags.items():
            mlflow.set_tag(key, value)