"""MLOpså®Ÿé¨“å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
import mlflow
import hydra
from omegaconf import DictConfig

# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
from src.utils.data_utils import get_dataset_name, detect_task_type, load_csv_data
from src.utils.cv_utils import create_cv_strategy

# componentsæ©Ÿèƒ½ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.mlops.components.pipeline import create_pipeline
from src.mlops.components.visualization import create_visualizations
from src.mlops.components.optimization import OptunaOptimizer
from src.mlops.components.artifacts import save_model_artifacts, log_experiment_metrics, setup_mlflow_experiment, set_mlflow_tags, log_config_parameters, log_runtime_parameters, create_prediction_dataframe, save_prediction_results
from src.utils.pipeline_utils import get_pipeline_feature_names

# matplotlibè¨­å®š
import os
import logging
os.environ['MPLBACKEND'] = 'Agg'

# matplotlibãƒ•ã‚©ãƒ³ãƒˆè­¦å‘ŠæŠ‘åˆ¶
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

import matplotlib
# DejaVu Sansãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Ubuntu']

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œ
try:
    import japanize_matplotlib
except ImportError:
    pass

# ä¸€èˆ¬çš„ãªè­¦å‘ŠæŠ‘åˆ¶
import warnings
warnings.filterwarnings('ignore')


@hydra.main(version_base=None, config_path="../../config", config_name="config")
def main(cfg: DictConfig):
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("âœ… Configé§†å‹•MLOpsãƒ•ãƒ­ãƒ¼é–‹å§‹")

    # MLflowå®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    setup_mlflow_experiment(cfg)

    # æ—¢å­˜runãŒã‚ã‚‹å ´åˆã¯çµ‚äº†
    if mlflow.active_run():
        mlflow.end_run()

    # ã‚«ã‚¹ã‚¿ãƒ Runåè¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    run_name = getattr(cfg.mlflow, 'run_id', None)

    with mlflow.start_run(run_name=run_name):
        # ã‚¿ã‚°è¨­å®šï¼ˆruné–‹å§‹å¾Œï¼‰
        set_mlflow_tags(cfg)

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df, feature_cols, target_names = load_csv_data(cfg)

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X = df[feature_cols]
        y = df[cfg.data.target_column]
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=cfg.data.test_size,
            random_state=cfg.data.random_state
        )

        print(f"ğŸ” Task Type: {detect_task_type(y)}")
        task_type = detect_task_type(y)
        print(f"ğŸ” X_train type: {type(X_train)}, shape: {X_train.shape}, columns: {list(X_train.columns)}")

        # Optunaæœ€é©åŒ–ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if cfg.optuna.enabled:
            optimizer = OptunaOptimizer(cfg, X_train, y_train, task_type)
            best_params, best_score = optimizer.optimize()
            print(f"ğŸ¯ Optuna best_params: {best_params}")
        else:
            best_params = {}
            print(f"âš ï¸ Optunaç„¡åŠ¹: best_params = {best_params}")

        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆbest_paramsã‚’åæ˜ ï¼‰
        passed_params = best_params if best_params else None
        print(f"ğŸ“¦ create_pipelineå‘¼ã³å‡ºã—: best_params={passed_params}")
        best_pipeline = create_pipeline(cfg, best_params=passed_params)

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±è¡¨ç¤º
        print(f"ğŸ”§ Pipeline: {best_pipeline} | ğŸ“Š Train: {len(X_train)} Test: {len(X_test)}")

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å­¦ç¿’
        best_pipeline.fit(X_train, y_train)

        # å®Ÿè¡Œæ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€æœ€é©åŒ–çµæœãªã©ï¼‰
        log_runtime_parameters(best_pipeline, cfg, best_params)

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ï¼ˆ1å›ã®ã¿å®Ÿè¡Œï¼‰
        y_pred = best_pipeline.predict(X_test)

        # CVè©•ä¾¡ã¾ãŸã¯Optunaçµæœä½¿ç”¨
        scoring = cfg.optuna.scoring.classification if task_type == "classification" else cfg.optuna.scoring.regression
        n_splits = cfg.evaluation.cv_strategy.params.n_splits

        if not cfg.optuna.enabled:
            cv_strategy = create_cv_strategy(cfg)
            print(f"ğŸ”„ CV Strategy: {cfg.evaluation.cv_strategy['class']} (n_splits={n_splits})")
            cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)
        else:
            cv_scores = np.array([best_score] * n_splits)
            print(f"ğŸ”„ CVè©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆOptunaæœ€é©åŒ–æ¸ˆã¿: {best_score:.3f}ï¼‰")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        log_experiment_metrics(best_pipeline, X_train, y_train, X_test, y_test, task_type, cv_scores, cfg=cfg, y_pred=y_pred)

        # äºˆæ¸¬çµæœDataFrameä½œæˆã¨ä¿å­˜
        df_predictions = create_prediction_dataframe(best_pipeline, X_test, y_test, task_type, y_pred=y_pred)
        save_prediction_results(df_predictions, cfg)

        # å¯è¦–åŒ–ç”Ÿæˆ
        if cfg.visualization.enabled:
            target_names_str = [str(name) for name in target_names]
            create_visualizations(
                best_pipeline, X_train, y_train, X_test, y_test,
                target_names_str, cfg.visualization.plots, cfg, task_type
            )

        # ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜
        transformed_feature_names = get_pipeline_feature_names(best_pipeline, feature_cols)
        save_model_artifacts(best_pipeline, transformed_feature_names, target_names, cfg)

        print("âœ… MLOpsãƒ•ãƒ­ãƒ¼å®Œäº†")


if __name__ == "__main__":
    main()